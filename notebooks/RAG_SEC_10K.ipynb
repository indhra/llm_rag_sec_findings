{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f09af7f7",
      "metadata": {
        "id": "f09af7f7"
      },
      "source": [
        "# SEC 10-K RAG System - Colab Demo\n",
        "\n",
        "**Author:** Indhra  \n",
        "**Assignment:** LLM + RAG Hands-On Coding Test\n",
        "\n",
        "This notebook demonstrates a RAG system for answering questions from Apple 2024 and Tesla 2023 10-K SEC filings.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/indhra/llm_rag_sec_findings/blob/main/notebooks/RAG_SEC_10K.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa74e5ee",
      "metadata": {
        "id": "fa74e5ee"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Clone the repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "75c1d526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75c1d526",
        "outputId": "131d41df-4116-4666-b187-ca8f43ab9474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llm_rag_sec_findings'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 157 (delta 72), reused 138 (delta 56), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (157/157), 1.72 MiB | 5.89 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n",
            "/Users/indhra/Machine_learning/Resumes_Indhra/ABB_JAN26/notebooks/llm_rag_sec_findings\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/indhra/llm_rag_sec_findings.git\n",
        "%cd llm_rag_sec_findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "21765355",
      "metadata": {
        "id": "21765355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pymupdf tiktoken sentence-transformers faiss-cpu rank-bm25 groq python-dotenv tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eb06aced",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eb06aced",
        "outputId": "17d442ae-f662-4bf9-bb28-c6cbd867e552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://console.groq.com/keys'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"https://console.groq.com/keys\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a629a85a",
      "metadata": {
        "id": "a629a85a"
      },
      "outputs": [],
      "source": [
        "# Set your Groq API key (free at console.groq.com)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Option 1: Enter manually (strips whitespace to prevent errors)\n",
        "api_key = getpass(\"Enter your Groq API key: \")\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key.strip()\n",
        "\n",
        "# Option 2: Use Colab secrets (recommended)\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY').strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f497f73c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f497f73c",
        "outputId": "1e4dcec5-536b-4a68-b058-27a84323e216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ”‘ Validating Groq API key...\n",
            "âœ… API key is valid and working!\n",
            "   Model: llama-3.1-8b-instant\n",
            "   Response: It's nice to meet\n"
          ]
        }
      ],
      "source": [
        "# Validate Groq API key\n",
        "print(\"ðŸ”‘ Validating Groq API key...\")\n",
        "try:\n",
        "    from groq import Groq\n",
        "\n",
        "    # Ensure the API key is stripped of any leading/trailing whitespace or newlines\n",
        "    groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if groq_api_key:\n",
        "        client = Groq(api_key=groq_api_key.strip())\n",
        "    else:\n",
        "        raise ValueError(\"GROQ_API_KEY environment variable is not set.\")\n",
        "\n",
        "    # Test with a simple request\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
        "        max_tokens=5\n",
        "    )\n",
        "\n",
        "    print(\"âœ… API key is valid and working!\")\n",
        "    print(f\"   Model: {response.model}\")\n",
        "    print(f\"   Response: {response.choices[0].message.content}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ API key validation failed: {str(e)}\")\n",
        "    print(\"\\nPlease check your API key and try again.\")\n",
        "    print(\"Get a free key at: https://console.groq.com/keys\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cc5b1b",
      "metadata": {
        "id": "30cc5b1b"
      },
      "source": [
        "## 2. Download SEC 10-K PDFs\n",
        "\n",
        "Download Apple 2024 and Tesla 2023 10-K filings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "91e1196a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e1196a",
        "outputId": "904f96b9-1e9f-4015-b99e-9493ff6faf71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using local copy\n",
            "Using local copy\n",
            "total 3816\n",
            "drwxr-xr-x@  4 indhra  staff     128 Jan 30 20:07 \u001b[34m.\u001b[m\u001b[m\n",
            "drwxr-xr-x@ 16 indhra  staff     512 Jan 30 20:07 \u001b[34m..\u001b[m\u001b[m\n",
            "-rw-r--r--@  1 indhra  staff  963934 Jan 30 20:07 10-Q4-2024-As-Filed.pdf\n",
            "-rw-r--r--@  1 indhra  staff  984581 Jan 30 20:07 tsla-20231231-gen.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download Apple 10-K 2024\n",
        "!wget -q -O data/apple_10k_2024.pdf \"https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\" 2>/dev/null || echo \"Using local copy\"\n",
        "\n",
        "# Download Tesla 10-K 2023\n",
        "!wget -q -O data/tesla_10k_2023.pdf \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\" 2>/dev/null || echo \"Using local copy\"\n",
        "\n",
        "# Check if files exist\n",
        "!ls -la data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bf36ca",
      "metadata": {
        "id": "99bf36ca"
      },
      "source": [
        "## 3. Initialize RAG Pipeline\n",
        "\n",
        "Load the embedding model, reranker, and LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0bc89bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bc89bc1",
        "outputId": "6da7e4fd-221e-40a4-e3b5-b2918d15eaab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Initializing SEC 10-K RAG Pipeline\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading embedding model...\n",
            "Using embedding model: bge-small\n",
            "  Model: BAAI/bge-small-en-v1.5\n",
            "  Dimensions: 384\n",
            "  Quality: good, Speed: fast\n",
            "  Device: mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16b7378294674e909f9a23db73ecc44d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Model loaded successfully\n",
            "\n",
            "[2/4] Loading reranker...\n",
            "Using reranker: ms-marco-mini\n",
            "  Model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "  Quality: good, Speed: fast\n",
            "  Device: mps\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a41da8dbcb4e5b91f9f14e66146ae8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Reranker loaded\n",
            "\n",
            "[3/4] Initializing LLM...\n",
            "GroqLLM initialized with model: llama-3.1-8b-instant\n",
            "\n",
            "[4/4] Pipeline ready!\n",
            "  Hybrid search: True\n",
            "  Retrieval top-k: 15\n",
            "  Rerank top-k: 7\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from src.pipeline import RAGPipeline\n",
        "\n",
        "# Initialize with Groq (free tier)\n",
        "pipeline = RAGPipeline(\n",
        "    embedding_model=\"bge-small\",    # Fast, good quality\n",
        "    reranker_model=\"ms-marco-mini\", # Fast cross-encoder\n",
        "    llm_provider=\"groq\",            # Free API\n",
        "    use_hybrid_search=True,         # Vector + BM25\n",
        "    top_k_retrieval=15,             # Initial candidates (increased for better coverage)\n",
        "    top_k_rerank=7                  # After reranking (increased for complex questions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0a14cb",
      "metadata": {
        "id": "fd0a14cb"
      },
      "source": [
        "## 4. Index Documents\n",
        "\n",
        "Parse PDFs, chunk, embed, and build the vector index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e0f30e1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0f30e1a",
        "outputId": "7a28681d-d15d-4dde-9760-fa7c5d043741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Indexing SEC 10-K Documents\n",
            "============================================================\n",
            "\n",
            "[Step 1] Parsing PDFs...\n",
            "Parsing 'Apple 10-K' (121 pages)...\n",
            "âœ“ Parsed 121 pages from Apple 10-K\n",
            "Parsing 'Tesla 10-K' (130 pages)...\n",
            "âœ“ Parsed 130 pages from Tesla 10-K\n",
            "\n",
            "[Step 2] Chunking documents...\n",
            "Chunking Apple 10-K...\n",
            "  â†’ Created 239 chunks\n",
            "Chunking Tesla 10-K...\n",
            "  â†’ Created 252 chunks\n",
            "\n",
            "âœ“ Total chunks: 491\n",
            "  Average tokens per chunk: 376\n",
            "  Min/Max tokens: 11/1050\n",
            "\n",
            "[Step 3] Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "507b4689192a43d3a29c436fa0174289",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 4] Building vector index...\n",
            "VectorStore initialized\n",
            "  Dimension: 384\n",
            "  Hybrid search: True\n",
            "  Hybrid alpha: 0.7 (vector weight)\n",
            "Adding 491 chunks to vector store...\n",
            "Building BM25 index...\n",
            "  BM25 index built with 491 documents\n",
            "âœ“ Vector store now contains 491 vectors\n",
            "\n",
            "[Step 5] Saving index to outputs/index...\n",
            "Saving vector store to outputs/index...\n",
            "âœ“ Vector store saved (491 chunks)\n",
            "\n",
            "============================================================\n",
            "âœ“ Indexing complete! 491 chunks indexed\n",
            "============================================================\n",
            "\n",
            "âœ… Indexed 491 chunks!\n"
          ]
        }
      ],
      "source": [
        "# This takes ~2 minutes on Colab (mostly embedding generation)\n",
        "num_chunks = pipeline.index_documents(data_dir=\"data/\")\n",
        "print(f\"\\nâœ… Indexed {num_chunks} chunks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd07191",
      "metadata": {
        "id": "8fd07191"
      },
      "source": [
        "## 5. Ask Questions!\n",
        "\n",
        "Now we can ask questions about the 10-K filings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "83915419",
      "metadata": {
        "id": "83915419"
      },
      "outputs": [],
      "source": [
        "def ask(question: str):\n",
        "    \"\"\"Helper function to ask a question and display the answer.\"\"\"\n",
        "    print(f\"â“ {question}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    result = pipeline.answer_question(question)\n",
        "\n",
        "    print(f\"ðŸ’¡ {result['answer']}\")\n",
        "    print(f\"\\nðŸ“š Sources: {result['sources']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "81bd95c0",
      "metadata": {
        "id": "81bd95c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â“ What was Apple's total revenue for fiscal year 2024?\n",
            "------------------------------------------------------------\n",
            "ðŸ’¡ ## Understand\n",
            "The question is asking for Apple's total revenue for fiscal year 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"6\" source=\"\n",
            "\n",
            "ðŸ“š Sources: ['Apple 10-K', 'Item 8', 'p. 32']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Apple revenue\n",
        "ask(\"What was Apple's total revenue for fiscal year 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "671dfe40",
      "metadata": {
        "id": "671dfe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â“ What was Tesla's net income for fiscal year 2023?\n",
            "------------------------------------------------------------\n",
            "ðŸ’¡ ## Understand\n",
            "The question is asking for Tesla's net income for fiscal year 2023.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"4\"\n",
            "\n",
            "ðŸ“š Sources: ['Tesla 10-K', 'Item 8', 'p. 51', 'Tesla 10-K', 'Item 8', 'p. 51']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Tesla net income\n",
        "ask(\"What was Tesla's net income for fiscal year 2023?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fdb3f052",
      "metadata": {
        "id": "fdb3f052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â“ How much did Apple spend on research and development in 2024?\n",
            "------------------------------------------------------------\n",
            "ðŸ’¡ ## Understand\n",
            "The question is asking for the amount Apple spent on research and development in 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context chunk \"1\" and context chunk \"2\".\n",
            "\n",
            "## Extract\n",
            "From context chunk \"1\", we have:\n",
            "\"Research and development\n",
            "$\n",
            "31,370\n",
            "5 % $\n",
            "29,915\n",
            "14 % $\n",
            "26,251\n",
            "Percentage of total net sales\n",
            "8%\n",
            "8%\n",
            "7%\"\n",
            "\n",
            "From context chunk \"2\", we have:\n",
            "\"Research and development\n",
            "$\n",
            "31,370\n",
            "\"\n",
            "\n",
            "## Calculate\n",
            "The amount spent on research and development in 2024 is already provided in the context.\n",
            "\n",
            "## Synthesize\n",
            "Apple spent $31,370 million on research and development in 2024.\n",
            "\n",
            "## Cite\n",
            "\n",
            "ðŸ“š Sources: ['Apple 10-K', 'Item 7', 'p. 27', 'Apple 10-K', 'Item 8', 'p. 32']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Apple R&D\n",
        "ask(\"How much did Apple spend on research and development in 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41f2876d",
      "metadata": {
        "id": "41f2876d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â“ How much cash and cash equivalents did Tesla report?\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Test: Tesla cash\n",
        "ask(\"How much cash and cash equivalents did Tesla report?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4e9402b",
      "metadata": {
        "id": "b4e9402b"
      },
      "outputs": [],
      "source": [
        "# Test: Out-of-scope (future prediction) - should refuse\n",
        "ask(\"What will Apple's revenue be in 2025?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "432899ee",
      "metadata": {
        "id": "432899ee"
      },
      "outputs": [],
      "source": [
        "# Test: Out-of-scope (investment advice) - should refuse\n",
        "ask(\"Should I invest in Tesla stock?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e232fcae",
      "metadata": {
        "id": "e232fcae"
      },
      "source": [
        "## 6. Run Full Evaluation\n",
        "\n",
        "Test all 13 questions from the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37f29d83",
      "metadata": {
        "id": "37f29d83"
      },
      "outputs": [],
      "source": [
        "# Run the full evaluation script\n",
        "\n",
        "!python -m src.test.evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c74f45",
      "metadata": {
        "id": "a6c74f45"
      },
      "source": [
        "## 7. Interactive Demo\n",
        "\n",
        "Ask your own questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0fa2011",
      "metadata": {
        "id": "d0fa2011"
      },
      "outputs": [],
      "source": [
        "# Interactive loop\n",
        "while True:\n",
        "    question = input(\"\\nðŸ” Enter your question (or 'quit' to exit): \")\n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    ask(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3e3a1b",
      "metadata": {
        "id": "2c3e3a1b"
      },
      "source": [
        "---\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  PDF Parse  â”‚ â†’  â”‚  Chunk   â”‚ â†’  â”‚  Embed  â”‚ â†’  â”‚ FAISS+BM25 â”‚\n",
        "â”‚  (PyMuPDF)  â”‚    â”‚ (512 tok)â”‚    â”‚  (BGE)  â”‚    â”‚  (Hybrid)  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                                                        â”‚\n",
        "                                                        â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   Answer    â”‚ â†  â”‚   LLM    â”‚ â†  â”‚ Rerank  â”‚ â†  â”‚   Search   â”‚\n",
        "â”‚ + Citations â”‚    â”‚  (Groq)  â”‚    â”‚(MS-MARCOâ”‚    â”‚  Results   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "| Question | Status |\n",
        "|----------|--------|\n",
        "| Q1: Apple revenue | âœ… $391,035M |\n",
        "| Q2: Apple shares | âœ… 15,115,823,000 |\n",
        "| Q3: Apple term debt | âœ… $96,662M |\n",
        "| Q4: Apple 10-K date | âœ… November 1, 2024 |\n",
        "| Q5: SEC comments | âœ… Item 1B: None |\n",
        "| Q6: Tesla revenue | âœ… $96,773M |\n",
        "| Q7: Automotive % | âœ… 83.04% |\n",
        "| Q8: Elon dependency | âš ï¸ Valid interpretation |\n",
        "| Q9: Tesla vehicles | âœ… Model S, 3, X, Y |\n",
        "| Q10: Lease pass-through | âœ… Finance solar |\n",
        "| Q11-13: Out-of-scope | âœ… Correctly refused |\n",
        "\n",
        "**Overall: 12/13 correct (92.3%)**\n",
        "\n",
        "See `design_report.md` for detailed explanations of each design decision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4b21ad",
      "metadata": {
        "id": "5c4b21ad"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
