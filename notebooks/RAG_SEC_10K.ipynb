{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f09af7f7",
      "metadata": {
        "id": "f09af7f7"
      },
      "source": [
        "# SEC 10-K RAG System - Colab Demo\n",
        "\n",
        "**Author:** Indhra  \n",
        "**Assignment:** LLM + RAG Hands-On Coding Test\n",
        "\n",
        "This notebook demonstrates a RAG system for answering questions from Apple 2024 and Tesla 2023 10-K SEC filings.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/indhra/llm_rag_sec_findings/blob/main/notebooks/RAG_SEC_10K.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa74e5ee",
      "metadata": {
        "id": "fa74e5ee"
      },
      "source": [
        "## 1. Setup\n",
        "\n",
        "Clone the repository and install dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "75c1d526",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75c1d526",
        "outputId": "131d41df-4116-4666-b187-ca8f43ab9474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'llm_rag_sec_findings'...\n",
            "remote: Enumerating objects: 157, done.\u001b[K\n",
            "remote: Counting objects: 100% (157/157), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 157 (delta 72), reused 138 (delta 56), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (157/157), 1.72 MiB | 5.89 MiB/s, done.\n",
            "Resolving deltas: 100% (72/72), done.\n",
            "/Users/indhra/Machine_learning/Resumes_Indhra/ABB_JAN26/notebooks/llm_rag_sec_findings\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "!git clone https://github.com/indhra/llm_rag_sec_findings.git\n",
        "%cd llm_rag_sec_findings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "21765355",
      "metadata": {
        "id": "21765355"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install -q pymupdf tiktoken sentence-transformers faiss-cpu rank-bm25 groq python-dotenv tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "eb06aced",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eb06aced",
        "outputId": "17d442ae-f662-4bf9-bb28-c6cbd867e552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://console.groq.com/keys'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"https://console.groq.com/keys\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a629a85a",
      "metadata": {
        "id": "a629a85a"
      },
      "outputs": [],
      "source": [
        "# Set your Groq API key (free at console.groq.com)\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# Option 1: Enter manually (strips whitespace to prevent errors)\n",
        "api_key = getpass(\"Enter your Groq API key: \")\n",
        "os.environ[\"GROQ_API_KEY\"] = api_key.strip()\n",
        "\n",
        "# Option 2: Use Colab secrets (recommended)\n",
        "# from google.colab import userdata\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY').strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f497f73c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f497f73c",
        "outputId": "1e4dcec5-536b-4a68-b058-27a84323e216"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Validating Groq API key...\n",
            "‚úÖ API key is valid and working!\n",
            "   Model: llama-3.1-8b-instant\n",
            "   Response: It's nice to meet\n"
          ]
        }
      ],
      "source": [
        "# Validate Groq API key\n",
        "print(\"üîë Validating Groq API key...\")\n",
        "try:\n",
        "    from groq import Groq\n",
        "\n",
        "    # Ensure the API key is stripped of any leading/trailing whitespace or newlines\n",
        "    groq_api_key = os.environ.get(\"GROQ_API_KEY\")\n",
        "    if groq_api_key:\n",
        "        client = Groq(api_key=groq_api_key.strip())\n",
        "    else:\n",
        "        raise ValueError(\"GROQ_API_KEY environment variable is not set.\")\n",
        "\n",
        "    # Test with a simple request\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"llama-3.1-8b-instant\",\n",
        "        messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
        "        max_tokens=5\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ API key is valid and working!\")\n",
        "    print(f\"   Model: {response.model}\")\n",
        "    print(f\"   Response: {response.choices[0].message.content}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå API key validation failed: {str(e)}\")\n",
        "    print(\"\\nPlease check your API key and try again.\")\n",
        "    print(\"Get a free key at: https://console.groq.com/keys\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30cc5b1b",
      "metadata": {
        "id": "30cc5b1b"
      },
      "source": [
        "## 2. Download SEC 10-K PDFs\n",
        "\n",
        "Download Apple 2024 and Tesla 2023 10-K filings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "91e1196a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91e1196a",
        "outputId": "904f96b9-1e9f-4015-b99e-9493ff6faf71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using local copy\n",
            "Using local copy\n",
            "total 3816\n",
            "drwxr-xr-x@  4 indhra  staff     128 Jan 30 20:07 \u001b[34m.\u001b[m\u001b[m\n",
            "drwxr-xr-x@ 16 indhra  staff     512 Jan 30 20:07 \u001b[34m..\u001b[m\u001b[m\n",
            "-rw-r--r--@  1 indhra  staff  963934 Jan 30 20:07 10-Q4-2024-As-Filed.pdf\n",
            "-rw-r--r--@  1 indhra  staff  984581 Jan 30 20:07 tsla-20231231-gen.pdf\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download Apple 10-K 2024\n",
        "!wget -q -O data/apple_10k_2024.pdf \"https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\" 2>/dev/null || echo \"Using local copy\"\n",
        "\n",
        "# Download Tesla 10-K 2023\n",
        "!wget -q -O data/tesla_10k_2023.pdf \"https://www.sec.gov/Archives/edgar/data/1318605/000162828024002390/tsla-20231231.htm\" 2>/dev/null || echo \"Using local copy\"\n",
        "\n",
        "# Check if files exist\n",
        "!ls -la data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bf36ca",
      "metadata": {
        "id": "99bf36ca"
      },
      "source": [
        "## 3. Initialize RAG Pipeline\n",
        "\n",
        "Load the embedding model, reranker, and LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0bc89bc1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bc89bc1",
        "outputId": "6da7e4fd-221e-40a4-e3b5-b2918d15eaab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Initializing SEC 10-K RAG Pipeline\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading embedding model...\n",
            "Using embedding model: bge-small\n",
            "  Model: BAAI/bge-small-en-v1.5\n",
            "  Dimensions: 384\n",
            "  Quality: good, Speed: fast\n",
            "  Device: mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16b7378294674e909f9a23db73ecc44d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertModel LOAD REPORT from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Model loaded successfully\n",
            "\n",
            "[2/4] Loading reranker...\n",
            "Using reranker: ms-marco-mini\n",
            "  Model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "  Quality: good, Speed: fast\n",
            "  Device: mps\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20a41da8dbcb4e5b91f9f14e66146ae8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading weights:   0%|          | 0/105 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | UNEXPECTED |  | \n",
            "\n",
            "Notes:\n",
            "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Reranker loaded\n",
            "\n",
            "[3/4] Initializing LLM...\n",
            "GroqLLM initialized with model: llama-3.1-8b-instant\n",
            "\n",
            "[4/4] Pipeline ready!\n",
            "  Hybrid search: True\n",
            "  Retrieval top-k: 15\n",
            "  Rerank top-k: 7\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "from src.pipeline import RAGPipeline\n",
        "\n",
        "# Initialize with Groq (free tier)\n",
        "pipeline = RAGPipeline(\n",
        "    embedding_model=\"bge-small\",    # Fast, good quality\n",
        "    reranker_model=\"ms-marco-mini\", # Fast cross-encoder\n",
        "    llm_provider=\"groq\",            # Free API\n",
        "    use_hybrid_search=True,         # Vector + BM25\n",
        "    top_k_retrieval=15,             # Initial candidates (increased for better coverage)\n",
        "    top_k_rerank=7                  # After reranking (increased for complex questions)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0a14cb",
      "metadata": {
        "id": "fd0a14cb"
      },
      "source": [
        "## 4. Index Documents\n",
        "\n",
        "Parse PDFs, chunk, embed, and build the vector index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e0f30e1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0f30e1a",
        "outputId": "7a28681d-d15d-4dde-9760-fa7c5d043741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Indexing SEC 10-K Documents\n",
            "============================================================\n",
            "\n",
            "[Step 1] Parsing PDFs...\n",
            "Parsing 'Apple 10-K' (121 pages)...\n",
            "‚úì Parsed 121 pages from Apple 10-K\n",
            "Parsing 'Tesla 10-K' (130 pages)...\n",
            "‚úì Parsed 130 pages from Tesla 10-K\n",
            "\n",
            "[Step 2] Chunking documents...\n",
            "Chunking Apple 10-K...\n",
            "  ‚Üí Created 239 chunks\n",
            "Chunking Tesla 10-K...\n",
            "  ‚Üí Created 252 chunks\n",
            "\n",
            "‚úì Total chunks: 491\n",
            "  Average tokens per chunk: 376\n",
            "  Min/Max tokens: 11/1050\n",
            "\n",
            "[Step 3] Generating embeddings...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "507b4689192a43d3a29c436fa0174289",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 4] Building vector index...\n",
            "VectorStore initialized\n",
            "  Dimension: 384\n",
            "  Hybrid search: True\n",
            "  Hybrid alpha: 0.7 (vector weight)\n",
            "Adding 491 chunks to vector store...\n",
            "Building BM25 index...\n",
            "  BM25 index built with 491 documents\n",
            "‚úì Vector store now contains 491 vectors\n",
            "\n",
            "[Step 5] Saving index to outputs/index...\n",
            "Saving vector store to outputs/index...\n",
            "‚úì Vector store saved (491 chunks)\n",
            "\n",
            "============================================================\n",
            "‚úì Indexing complete! 491 chunks indexed\n",
            "============================================================\n",
            "\n",
            "‚úÖ Indexed 491 chunks!\n"
          ]
        }
      ],
      "source": [
        "# This takes ~2 minutes on Colab (mostly embedding generation)\n",
        "num_chunks = pipeline.index_documents(data_dir=\"data/\")\n",
        "print(f\"\\n‚úÖ Indexed {num_chunks} chunks!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8fd07191",
      "metadata": {
        "id": "8fd07191"
      },
      "source": [
        "## 5. Ask Questions!\n",
        "\n",
        "Now we can ask questions about the 10-K filings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "83915419",
      "metadata": {
        "id": "83915419"
      },
      "outputs": [],
      "source": [
        "def ask(question: str):\n",
        "    \"\"\"Helper function to ask a question and display the answer.\"\"\"\n",
        "    print(f\"‚ùì {question}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    result = pipeline.answer_question(question)\n",
        "\n",
        "    print(f\"üí° {result['answer']}\")\n",
        "    print(f\"\\nüìö Sources: {result['sources']}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "81bd95c0",
      "metadata": {
        "id": "81bd95c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì What was Apple's total revenue for fiscal year 2024?\n",
            "------------------------------------------------------------\n",
            "üí° ## Understand\n",
            "The question is asking for Apple's total revenue for fiscal year 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"6\" source=\"\n",
            "\n",
            "üìö Sources: ['Apple 10-K', 'Item 8', 'p. 32']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Apple revenue\n",
        "ask(\"What was Apple's total revenue for fiscal year 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "671dfe40",
      "metadata": {
        "id": "671dfe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì What was Tesla's net income for fiscal year 2023?\n",
            "------------------------------------------------------------\n",
            "üí° ## Understand\n",
            "The question is asking for Tesla's net income for fiscal year 2023.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"4\"\n",
            "\n",
            "üìö Sources: ['Tesla 10-K', 'Item 8', 'p. 51', 'Tesla 10-K', 'Item 8', 'p. 51']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Tesla net income\n",
        "ask(\"What was Tesla's net income for fiscal year 2023?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fdb3f052",
      "metadata": {
        "id": "fdb3f052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì How much did Apple spend on research and development in 2024?\n",
            "------------------------------------------------------------\n",
            "üí° ## Understand\n",
            "The question is asking for the amount Apple spent on research and development in 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context chunk \"1\" and context chunk \"2\".\n",
            "\n",
            "## Extract\n",
            "From context chunk \"1\", we have:\n",
            "\"Research and development\n",
            "$\n",
            "31,370\n",
            "5 % $\n",
            "29,915\n",
            "14 % $\n",
            "26,251\n",
            "Percentage of total net sales\n",
            "8%\n",
            "8%\n",
            "7%\"\n",
            "\n",
            "From context chunk \"2\", we have:\n",
            "\"Research and development\n",
            "$\n",
            "31,370\n",
            "\"\n",
            "\n",
            "## Calculate\n",
            "The amount spent on research and development in 2024 is already provided in the context.\n",
            "\n",
            "## Synthesize\n",
            "Apple spent $31,370 million on research and development in 2024.\n",
            "\n",
            "## Cite\n",
            "\n",
            "üìö Sources: ['Apple 10-K', 'Item 7', 'p. 27', 'Apple 10-K', 'Item 8', 'p. 32']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Apple R&D\n",
        "ask(\"How much did Apple spend on research and development in 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "41f2876d",
      "metadata": {
        "id": "41f2876d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì How much cash and cash equivalents did Tesla report?\n",
            "------------------------------------------------------------\n",
            "üí° ## Understand\n",
            "The question is asking for the amount of cash and cash equivalents reported by Tesla.\n",
            "\n",
            "## Locate\n",
            "The relevant context chunks are:\n",
            "- <context id=\"1\" source=\"[Tesla 10-K, Item 8, p. 50]\"> (Consolidated Balance Sheets)\n",
            "- <context id=\"2\" source=\"[Tesla 10-K, Note 2, p. 65]\"> (Cash and cash equivalents and restricted cash)\n",
            "- <context id=\"5\" source=\"[Tesla 10-K, Note 2, p. 65]\"> (Cash and cash equivalents and restricted cash)\n",
            "\n",
            "## Extract\n",
            "From <context id=\"1\" source=\"[Tesla 10-K, Item 8, p. 50]\">, we have:\n",
            "Cash and cash equivalents = $16,398 million (as of December 31, 2023)\n",
            "\n",
            "From <context id=\"2\" source=\"[Tesla 10-K, Note 2, p. 65]\">, we have:\n",
            "Cash and cash equivalents = $16,398 million (as of December 31, 2023)\n",
            "Restricted cash included in prepaid expenses and other current assets = Not specified\n",
            "Restricted cash included in other non-current assets = Not specified\n",
            "\n",
            "From <context id=\"5\" source=\"[Tesla 10-K, Note 2, p. 65]\">, we have:\n",
            "Total cash and cash equivalents and restricted cash = $17,189 million (as of December 31, 2023)\n",
            "\n",
            "## Synthesize\n",
            "We have two different values for cash and cash equivalents:\n",
            "- $16,398 million (from <context id=\"1\" source=\"[Tesla 10-K, Item 8, p. 50]\"> and <context id=\"2\" source=\"[Tesla 10-K, Note 2, p. 65]\">)\n",
            "- $17,189 million (from <context id=\"5\" source=\"[Tesla 10-K, Note 2, p. 65]\">)\n",
            "\n",
            "Since <context id=\"5\" source=\"[Tesla 10-K, Note 2, p. 65]\"> includes both cash and cash equivalents and restricted cash, we will use this value as the total cash and cash equivalents.\n",
            "\n",
            "## Calculate\n",
            "No calculation is needed, as the value is already provided.\n",
            "\n",
            "## Cite\n",
            "[\"Tesla 10-K\", \"Note 2\", \"p. 65\"]\n",
            "\n",
            "## Answer\n",
            "Tesla reported $17,189 million in total cash and cash equivalents and restricted cash as of December 31, 2023.\n",
            "\n",
            "üìö Sources: []\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Tesla cash\n",
        "ask(\"How much cash and cash equivalents did Tesla report?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b4e9402b",
      "metadata": {
        "id": "b4e9402b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì What will Apple's revenue be in 2025?\n",
            "------------------------------------------------------------\n",
            "üí° This question cannot be answered based on the provided documents.\n",
            "\n",
            "üìö Sources: []\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Out-of-scope (future prediction) - should refuse\n",
        "ask(\"What will Apple's revenue be in 2025?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "432899ee",
      "metadata": {
        "id": "432899ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì Should I invest in Tesla stock?\n",
            "------------------------------------------------------------\n",
            "üí° **Understand**: The user is asking for investment advice on Tesla stock.\n",
            "**Locate**: The context does not contain explicit investment advice or recommendations.\n",
            "**Extract**: The context mentions potential risks and challenges facing Tesla, such as:\n",
            "* Litigation and regulatory issues (contexts 5, 6, and 7)\n",
            "* Competition for key employees (context 4)\n",
            "* Potential impact of government incentives and tax credits (context 3)\n",
            "**Synthesize**: Based on the provided context, there is no explicit recommendation to invest in Tesla stock. The context highlights potential risks and challenges that may affect the company's performance.\n",
            "**Cite**: [\"Tesla 10-K\", \"Note 15\", \"p. 90\"], [\"Tesla 10-K\", \"Item 1A\", \"p. 21\"], [\"Tesla 10-K\", \"Item 1A\", \"p. 26\"]\n",
            "\n",
            "**Answer**: I cannot provide investment advice, stock recommendations, or trading guidance. The context does not contain explicit information about Tesla's stock performance or investment potential.\n",
            "\n",
            "üìö Sources: ['Tesla 10-K', 'Note 15', 'p. 90', 'Tesla 10-K', 'Item 1A', 'p. 21', 'Tesla 10-K', 'Item 1A', 'p. 26']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test: Out-of-scope (investment advice) - should refuse\n",
        "ask(\"Should I invest in Tesla stock?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e232fcae",
      "metadata": {
        "id": "e232fcae"
      },
      "source": [
        "## 6. Run Full Evaluation\n",
        "\n",
        "Test all 13 questions from the assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "37f29d83",
      "metadata": {
        "id": "37f29d83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SEC 10-K RAG EVALUATION\n",
            "================================================================================\n",
            "\n",
            "Initializing pipeline...\n",
            "============================================================\n",
            "Initializing SEC 10-K RAG Pipeline\n",
            "============================================================\n",
            "\n",
            "[1/4] Loading embedding model...\n",
            "Using embedding model: bge-small\n",
            "  Model: BAAI/bge-small-en-v1.5\n",
            "  Dimensions: 384\n",
            "  Quality: good, Speed: fast\n",
            "  Device: mps\n",
            "Loading weights: 100%|‚ñà| 199/199 [00:00<00:00, 4217.79it/s, Materializing param=\n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: BAAI/bge-small-en-v1.5\n",
            "Key                     | Status     |  | \n",
            "------------------------+------------+--+-\n",
            "embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "‚úì Model loaded successfully\n",
            "\n",
            "[2/4] Loading reranker...\n",
            "Using reranker: ms-marco-mini\n",
            "  Model: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "  Quality: good, Speed: fast\n",
            "  Device: mps\n",
            "Loading weights: 100%|‚ñà| 105/105 [00:00<00:00, 2331.37it/s, Materializing param=\n",
            "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
            "Key                          | Status     |  | \n",
            "-----------------------------+------------+--+-\n",
            "bert.embeddings.position_ids | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "‚úì Reranker loaded\n",
            "\n",
            "[3/4] Initializing LLM...\n",
            "GroqLLM initialized with model: llama-3.1-8b-instant\n",
            "\n",
            "[4/4] Pipeline ready!\n",
            "  Hybrid search: True\n",
            "  Retrieval top-k: 15\n",
            "  Rerank top-k: 7\n",
            "============================================================\n",
            "Loading index from outputs/index...\n",
            "Loading vector store from outputs/index...\n",
            "VectorStore initialized\n",
            "  Dimension: 384\n",
            "  Hybrid search: True\n",
            "  Hybrid alpha: 0.7 (vector weight)\n",
            "‚úì Loaded 491 chunks\n",
            "‚úì Loaded 491 chunks\n",
            "Loaded existing index\n",
            "\n",
            "================================================================================\n",
            "RUNNING EVALUATION ON 13 QUESTIONS\n",
            "================================================================================\n",
            "\n",
            "[Q1] What was Apples total revenue for the fiscal year ended September 28, 2024?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question is asking for Apple's total revenue for the fiscal year ended September 28, 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in the Consolidated Statements of Operations in context id=\"1\".\n",
            "\n",
            "## Extract\n",
            "The total net sales for the fiscal year ended September 28, 2024, is $...\n",
            "Sources: ['Apple 10-K', 'Item 8', 'p. 32']\n",
            "\n",
            "[Q2] How many shares of common stock were issued and outstanding as of October 18, 2024?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question is asking for the number of shares of common stock issued and outstanding as of October 18, 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"5\" and context id=\"6\".\n",
            "\n",
            "## Extract\n",
            "According to context id=\"5\", the number of shares of common stock issued and out...\n",
            "Sources: ['Apple 10-K', 'Part III', 'p. 2', 'Apple 10-K', 'Part III', 'p. 2']\n",
            "\n",
            "[Q3] What is the total amount of term debt (current + non-current) reported by Apple as of September 28, 2024?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks for the total amount of term debt (current + non-current) reported by Apple as of September 28, 2024.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in the following context chunks:\n",
            "- <context id=\"1\" source=\"...\n",
            "Sources: ['Apple 10-K', 'Note 9', 'p. 46', 'Apple 10-K', 'Note 9', 'p. 46', 'Apple 10-K', 'Note 4', 'p. 41', 'Apple 10-K', 'Item 8', 'p. 34']\n",
            "\n",
            "[Q4] On what date was Apples 10-K report for 2024 signed and filed with the SEC?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks for the date Apple's 10-K report for 2024 was signed and filed with the SEC.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context chunk #2:...\n",
            "Sources: ['Apple 10-K', 'Item 10', 'p. 60', 'Apple 10-K', 'Item 10', 'p. 60']\n",
            "\n",
            "[Q5] Does Apple have any unresolved staff comments from the SEC as of this filing? How do you know?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question is asking if Apple has any unresolved staff comments from the SEC as of the filing.\n",
            "\n",
            "## Locate\n",
            "The relevant context chunk is <context id=\"1\" source=\"...\n",
            "Sources: ['Apple 10-K', 'Item 1B', 'p. 20', 'Apple 10-K', 'Item 1B', 'p. 20']\n",
            "\n",
            "[Q6] What was Teslas total revenue for the year ended December 31, 2023?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks for Tesla's total revenue for the year ended December 31, 2023.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in the Consolidated Statements of Operations (context id=\"1\").\n",
            "\n",
            "## Extract\n",
            "Total revenues for the year ended December 31, 2023, is $96,773 million. [\"Tesla 10-...\n",
            "Sources: ['Tesla 10-K', 'Item 8', 'p. 51', 'Tesla 10-K', 'Item 8', 'p. 51']\n",
            "\n",
            "[Q7] What percentage of Teslas total revenue in 2023 came from Automotive Sales (excluding Leasing)?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks for the percentage of Tesla's total revenue in 2023 that came from Automotive Sales (excluding Leasing).\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"4\" and context id=\"2\".\n",
            "\n",
            "## Extract\n",
            "From context id=\"4\", we have:\n",
            "Automotive sales revenue = $78,509 mil...\n",
            "Sources: ['Tesla 10-K', 'Item 1', 'p. 39', 'Tesla 10-K', 'Item 8', 'p. 51']\n",
            "\n",
            "[Q8] What is the primary reason Tesla states for being highly dependent on Elon Musk?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks about the primary reason Tesla states for being highly dependent on Elon Musk.\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context chunk \"1\" from the Tesla 10-K (FY2023).\n",
            "\n",
            "## Extract\n",
            "According to the document, Tesla is highly dependent on Elon Musk because he does...\n",
            "Sources: ['Tesla 10-K', 'Item 1A', 'p. 22']\n",
            "\n",
            "[Q9] What types of vehicles does Tesla currently produce and deliver?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks about the types of vehicles Tesla currently produces and delivers.\n",
            "\n",
            "## Locate\n",
            "Relevant information is found in context chunk <context id=\"7\"> and <context id=\"1\">.\n",
            "\n",
            "## Extract\n",
            "From context chunk <context id=\"7\">:\n",
            "\"We currently manufacture five different consumer vehic...\n",
            "Sources: ['Tesla 10-K', 'Item 1', 'p. 5', 'Tesla 10-K', 'Item 1', 'p. 35', 'Tesla 10-K', 'Item 1', 'p. 5', 'Tesla 10-K', 'Item 1', 'p. 35']\n",
            "\n",
            "[Q10] What is the purpose of Teslas 'lease pass-through fund arrangements'?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question is asking about the purpose of Tesla's \"lease pass-through fund arrangements\".\n",
            "\n",
            "## Locate\n",
            "The relevant information is found in context id=\"4\" and context id=\"5\".\n",
            "\n",
            "## Extract\n",
            "According to context id=\"4\", the lease pass-through fund arrangements are described as follows:\n",
            "\"Un...\n",
            "Sources: ['Tesla 10-K', 'Note 13', 'p. 82']\n",
            "\n",
            "[Q11] What is Teslas stock price forecast for 2025?\n",
            "------------------------------------------------------------\n",
            "Answer: This question cannot be answered based on the provided documents....\n",
            "Sources: []\n",
            "\n",
            "[Q12] Who is the CFO of Apple as of 2025?\n",
            "------------------------------------------------------------\n",
            "Answer: ## Understand\n",
            "The question asks for the Chief Financial Officer (CFO) of Apple as of 2025.\n",
            "\n",
            "## Locate\n",
            "The relevant context chunk is not explicitly mentioned in the provided context. However, we can infer that the CFO's information might be present in the context chunks related to the management or c...\n",
            "Sources: []\n",
            "\n",
            "[Q13] What color is Teslas headquarters painted?\n",
            "------------------------------------------------------------\n",
            "Answer: This question cannot be answered based on the provided documents....\n",
            "Sources: []\n",
            "\n",
            "================================================================================\n",
            "EVALUATION COMPLETE\n",
            "Results saved to: outputs/evaluation_results_20260130_202107.json\n",
            "================================================================================\n",
            "\n",
            "Output format:\n",
            "[\n",
            "  {\n",
            "    \"question_id\": 1,\n",
            "    \"answer\": \"## Understand\\nThe question is asking for Apple's total revenue for the fiscal year ended September 28, 2024.\\n\\n## Locate\\nThe relevant information is found in the Consolidated Statements of Operations in context id=\\\"1\\\".\\n\\n## Extract\\nThe total net sales for the fiscal year ended September 28, 2024, is $391,035 million.\\n\\n## Synthesize\\nThe total revenue for the fiscal year ended September 28, 2024, is $391,035 million.\\n\\n## Cite\\n[\\\"Apple 10-K\\\", \\\"Item 8\\\", \\\"p. 32\\\"]\\n\\nThe answer is $391,035 million.\",\n",
            "    \"sources\": [\n",
            "      \"Apple 10-K\",\n",
            "      \"Item 8\",\n",
            "      \"p. 32\"\n",
            "    ]\n",
            "  }\n",
            "]\n",
            "\n",
            "Summary:\n",
            "  Questions answered: 10\n",
            "  Questions refused/not found: 3\n"
          ]
        }
      ],
      "source": [
        "# Run the full evaluation script\n",
        "\n",
        "!python -m src.test.evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c74f45",
      "metadata": {
        "id": "a6c74f45"
      },
      "source": [
        "## 7. Interactive Demo\n",
        "\n",
        "Ask your own questions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d0fa2011",
      "metadata": {
        "id": "d0fa2011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùì \n",
            "------------------------------------------------------------\n",
            "üí° Since there is no user question provided, I will demonstrate the response structure with a sample question.\n",
            "\n",
            "**Sample Question:** What are the production locations and statuses of Tesla's announced vehicle models in production and under development, as of the date of this Annual Report on Form 10-K?\n",
            "\n",
            "**Understand:** The question is asking for the production locations and statuses of Tesla's vehicle models.\n",
            "\n",
            "**Locate:** The relevant information is found in context chunk 6: [\"Tesla 10-K\", \"Item 1\", \"p. 35\"]\n",
            "\n",
            "**Extract:** The production locations and statuses of Tesla's announced vehicle models are listed as follows:\n",
            "\n",
            "1. Fremont Factory\n",
            "\t* Model S / Model X: Active\n",
            "\t* Model 3 / Model Y: Active\n",
            "2. Gigafactory Shanghai\n",
            "\t* Model 3 / Model Y: Active\n",
            "3. Gigafactory Berlin-Brandenburg\n",
            "\t* Model Y: Active\n",
            "4. Gigafactory Texas\n",
            "\t* Model Y: Active\n",
            "\n",
            "**Synthesize:** The production locations and statuses of Tesla's announced vehicle models in production and under development, as of the date of this Annual Report on Form 10-K, are listed above.\n",
            "\n",
            "**Cite:** [\"Tesla 10-K\", \"Item 1\", \"p. 35\"]\n",
            "\n",
            "Please note that this is a demonstration of the response structure, and the actual question will be answered based on the user's input.\n",
            "\n",
            "üìö Sources: ['Tesla 10-K', 'Item 1', 'p. 35', 'Tesla 10-K', 'Item 1', 'p. 35']\n",
            "\n",
            "‚ùì \n",
            "------------------------------------------------------------\n",
            "üí° Since there is no user question provided, I will demonstrate the response structure with a sample question.\n",
            "\n",
            "**Sample Question:** What are the production locations and statuses of Tesla's announced vehicle models in production and under development, as of the date of this Annual Report on Form 10-K?\n",
            "\n",
            "**Understand:** The question asks for the production locations and statuses of Tesla's vehicle models.\n",
            "\n",
            "**Locate:** The relevant information is found in context chunk 6: [\"Tesla 10-K\", \"Item 1\", \"p. 35\"].\n",
            "\n",
            "**Extract:** The production locations and statuses of Tesla's announced vehicle models are as follows:\n",
            "\n",
            "1. Fremont Factory\n",
            "\t* Model S / Model X: Active\n",
            "\t* Model 3 / Model Y: Active\n",
            "2. Gigafactory Shanghai\n",
            "\t* Model 3 / Model Y: Active\n",
            "3. Gigafactory Berlin-Brandenburg\n",
            "\t* Model Y: Active\n",
            "4. Gigafactory Texas\n",
            "\t* Model Y: Active\n",
            "\n",
            "**Synthesize:** The production locations and statuses of Tesla's announced vehicle models are listed above.\n",
            "\n",
            "**Cite:** [\"Tesla 10-K\", \"Item 1\", \"p. 35\"]\n",
            "\n",
            "Note: This is a demonstration of the response structure. Please provide a user question for a real response.\n",
            "\n",
            "üìö Sources: ['Tesla 10-K', 'Item 1', 'p. 35', 'Tesla 10-K', 'Item 1', 'p. 35']\n",
            "\n",
            "‚ùì \n",
            "------------------------------------------------------------\n",
            "üí° Since there is no user question provided, I will demonstrate the response structure with a sample question.\n",
            "\n",
            "**Sample Question:** What is the production status of the Model Y at the Gigafactory Texas?\n",
            "\n",
            "**Understand:** The question is asking about the production status of the Model Y at the Gigafactory Texas.\n",
            "\n",
            "**Locate:** The relevant information is found in context chunk 6:\n",
            "\n",
            "üìö Sources: ['Tesla 10-K', 'Item 1', 'p. 35', 'Tesla 10-K', 'Item 1', 'p. 35', 'Tesla 10-K', 'Item 1', 'p. 35']\n",
            "\n",
            "‚ùì \n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/.venv/lib/python3.13/site-packages/groq/_base_client.py:1024\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/.venv/lib/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGoodbye!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mask\u001b[39m\u001b[34m(question)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùì \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m result = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43manswer_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müí° \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìö Sources: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33msources\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/notebooks/llm_rag_sec_findings/src/pipeline.py:392\u001b[39m, in \u001b[36mRAGPipeline.answer_question\u001b[39m\u001b[34m(self, query, question_id)\u001b[39m\n\u001b[32m    389\u001b[39m reranked = \u001b[38;5;28mself\u001b[39m.rerank(query, results)\n\u001b[32m    391\u001b[39m \u001b[38;5;66;03m# Generate\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreranked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    394\u001b[39m result = answer.to_dict()\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m question_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/notebooks/llm_rag_sec_findings/src/pipeline.py:332\u001b[39m, in \u001b[36mRAGPipeline.generate_answer\u001b[39m\u001b[34m(self, query, context_results)\u001b[39m\n\u001b[32m    321\u001b[39m context_chunks = [\n\u001b[32m    322\u001b[39m     {\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: r.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    328\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m context_results\n\u001b[32m    329\u001b[39m ]\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Generate answer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m llm_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;66;03m# Parse answer and sources\u001b[39;00m\n\u001b[32m    335\u001b[39m answer_text, parsed_sources = parse_answer_and_sources(llm_response.answer)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/notebooks/llm_rag_sec_findings/src/llm.py:333\u001b[39m, in \u001b[36mGroqLLM.generate\u001b[39m\u001b[34m(self, query, context_chunks, max_tokens, temperature)\u001b[39m\n\u001b[32m    330\u001b[39m start_time = time.time()\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYSTEM_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    343\u001b[39m     latency = (time.time() - start_time) * \u001b[32m1000\u001b[39m\n\u001b[32m    345\u001b[39m     answer = response.choices[\u001b[32m0\u001b[39m].message.content\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/.venv/lib/python3.13/site-packages/groq/resources/chat/completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/.venv/lib/python3.13/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/.venv/lib/python3.13/site-packages/groq/_base_client.py:1030\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1029\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Machine_learning/Resumes_Indhra/ABB_JAN26/.venv/lib/python3.13/site-packages/groq/_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1067\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1068\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Interactive loop\n",
        "while True:\n",
        "    question = input(\"\\nüîç Enter your question (or 'quit' to exit): \")\n",
        "    if question.lower() in ['quit', 'exit', 'q']:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    ask(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c3e3a1b",
      "metadata": {
        "id": "2c3e3a1b"
      },
      "source": [
        "---\n",
        "\n",
        "## System Architecture\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  PDF Parse  ‚îÇ ‚Üí  ‚îÇ  Chunk   ‚îÇ ‚Üí  ‚îÇ  Embed  ‚îÇ ‚Üí  ‚îÇ FAISS+BM25 ‚îÇ\n",
        "‚îÇ  (PyMuPDF)  ‚îÇ    ‚îÇ (512 tok)‚îÇ    ‚îÇ  (BGE)  ‚îÇ    ‚îÇ  (Hybrid)  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "                                                        ‚îÇ\n",
        "                                                        ‚ñº\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ   Answer    ‚îÇ ‚Üê  ‚îÇ   LLM    ‚îÇ ‚Üê  ‚îÇ Rerank  ‚îÇ ‚Üê  ‚îÇ   Search   ‚îÇ\n",
        "‚îÇ + Citations ‚îÇ    ‚îÇ  (Groq)  ‚îÇ    ‚îÇ(MS-MARCO‚îÇ    ‚îÇ  Results   ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "## Evaluation Results\n",
        "\n",
        "| Question | Status |\n",
        "|----------|--------|\n",
        "| Q1: Apple revenue | ‚úÖ $391,035M |\n",
        "| Q2: Apple shares | ‚úÖ 15,115,823,000 |\n",
        "| Q3: Apple term debt | ‚úÖ $96,662M |\n",
        "| Q4: Apple 10-K date | ‚úÖ November 1, 2024 |\n",
        "| Q5: SEC comments | ‚úÖ Item 1B: None |\n",
        "| Q6: Tesla revenue | ‚úÖ $96,773M |\n",
        "| Q7: Automotive % | ‚úÖ 83.04% |\n",
        "| Q8: Elon dependency | ‚ö†Ô∏è Valid interpretation |\n",
        "| Q9: Tesla vehicles | ‚úÖ Model S, 3, X, Y |\n",
        "| Q10: Lease pass-through | ‚úÖ Finance solar |\n",
        "| Q11-13: Out-of-scope | ‚úÖ Correctly refused |\n",
        "\n",
        "**Overall: 12/13 correct (92.3%)**\n",
        "\n",
        "See `design_report.md` for detailed explanations of each design decision."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c4b21ad",
      "metadata": {
        "id": "5c4b21ad"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
