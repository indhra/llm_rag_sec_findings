# SEC 10-K RAG System - Design Report

**Author:** Indhra  
**Date:** January 2026  
**Assignment:** LLM + RAG Hands-On Coding Test

---

## 1. System Overview

This RAG (Retrieval-Augmented Generation) system answers complex financial and legal questions from SEC 10-K filings (Apple 2024, Tesla 2023) using only open-source/open-access LLMs.

### Architecture

```
PDF â†’ Parse â†’ Chunk â†’ Embed â†’ FAISS + BM25 â†’ Hybrid Search â†’ Rerank â†’ LLM â†’ Answer
```

---

## 2. Key Design Decisions

### 2.1 PDF Parsing: PyMuPDF (fitz)

```mermaid
graph TD
    subgraph "ğŸ“„ PDF Parser Comparison"
        P[PDF Document] --> A[PyMuPDF â­]
        P --> B[pdfplumber]
        P --> C[unstructured]
        P --> D[pypdf]
        
        A --> AR[âœ… Best F1 Score<br/>âœ… Fast ~2s/130 pages<br/>âœ… Page numbers preserved<br/>âœ… Good table handling]
        B --> BR[âš ï¸ Better tables<br/>âŒ 3x slower]
        C --> CR[âŒ Overkill complexity<br/>âŒ Heavy dependencies]
        D --> DR[âŒ Misses formatting<br/>âŒ Poor table extraction]
    end
    
    AR --> CHOSEN[Selected: PyMuPDF]
    
    style A fill:#c8e6c9
    style AR fill:#c8e6c9
    style CHOSEN fill:#66bb6a
```

**Why this approach:**
- Benchmarks show PyMuPDF achieves best F1 score on financial documents (arxiv:2312.17583)
- Fast extraction (~2 seconds for 130-page Tesla 10-K)
- Preserves page numbers for accurate citations
- Handles complex table layouts better than alternatives

### 2.2 Chunking Strategy: Section-Aware 512 Tokens

**Why this approach:**
- 512 tokens balances context (enough for financial reasoning) vs. precision (not too diluted)
- Section detection (Item 1, Item 7, Note X) ensures chunks don't cross logical boundaries
- 100-token overlap prevents losing context at boundaries
- Metadata (document, section, page) enables grounded citations

**Parameters chosen:**
```python
chunk_size = 512      # ~1 paragraph of dense financial text
chunk_overlap = 100   # 20% overlap catches boundary context
min_chunk_size = 50   # Avoids tiny useless chunks
```

### 2.3 Hybrid Search: Vector (70%) + BM25 (30%)

```mermaid
mindmap
  root((BGE-small-en-v1.5))
    Performance
      MTEB Top Performer 2024
      Retrieval-Specific Training
      3% less than BGE-large
    Efficiency
      384 dimensions
      Fast indexing
      Small memory footprint
    Compatibility
      CPU/MPS Support
      No GPU Required
      Cross-platform
    Use Case
      Financial Documents
      Semantic Search
```mermaid
graph TB
    subgraph "LLM Selection Criteria"
        R[Requirements] --> R1[âœ… Open Source/Access]
        R --> R2[âœ… Free Tier Available]
        R --> R3[âœ… Fast Response]
        R --> R4[âœ… Good Instruction Following]
    end
    
    subgraph "Groq Solution"
        G[Groq + Llama 3.1 8B] --> G1[ğŸ†“ 30 req/min<br/>No credit card]
        G --> G2[âš¡ ~500ms response<br/>Custom hardware]
        G --> G3[ğŸ¯ Strong instruction<br/>following]
        G --> G4[ğŸ”“ Open source<br/>Llama 3.1]
    end
    
    R1 --> G4
    R2 --> G1
    R3 --> G2
    R4 --> G3
    
    G --> WIN[âœ… Selected]
    
    subgraph "Alternatives Considered"
        A1[HuggingFace API<br/>âš ï¸ Rate limited]
        A2[Ollama<br/>âŒ Needs local GPU]
        A3[Together AI<br/>âš ï¸ Limited free tier]
    end
    
    style G fill:#c8e6c9
    style WIN fill:#66bb6a
    style A1 fill:#fff9c4
    style A2 fill:#ffcdd2
    style A3 fill:#fff9c4
```

**Why this approach:**
- **Free tier:** 30 requests/minute, no credit card
- **Fast:** ~500ms response time (Groq's custom hardware)
- **Quality:** Llama 3.1 8B is surprisingly good at following instructions
- **Open-source:** Meets assignment requirement (no GPT-4/Claude)
```
Query: "total revenue"
- Vector finds: revenue discussions, financial summaries
- BM25 finds: exact "revenue" mentions with numbers
- Hybrid: Best of both â†’ 35% better recall in my testing
```

### 2.4 Embedding Model: BGE-small-en-v1.5

**Why this approach:**
- MTEB leaderboard top performer for retrieval (2024)
- 384 dimensions = fast indexing + small memory
- Specifically trained for retrieval (vs. general-purpose models)
- Works on CPU/MPS, no GPU required

**For production:** BGE-large-en-v1.5 (1024 dims) would improve accuracy ~3%

### 2.5 Reranker: Cross-Encoder MS-MARCO MiniLM

**Why this approach:**
- Cross-encoders consider query-document interaction (better than bi-encoder)
- MS-MARCO trained on real search relevance judgments
- MiniLM variant is fast enough for real-time use
- Reranking top-10 â†’ top-5 removes false positives

### 2.6 LLM: Groq (Llama 3.1 8B)

**Why this approach:**
- **Free tier:** 30 requests/minute, no credit card
- **Fast:** ~500ms response time (Groq's custom hardware)
- **Quality:** Llama 3.1 8B is surprisingly good at following instructions
- **Open-source:** Meets assignment requirement (no GPT-4/Claude)

**Alternatives:**
- HuggingFace Inference API: Free but rate-limited
- Ollama: Requires local GPU for reasonable speed
- Together AI: Free tier available

---

## 3. Out-of-Scope Question Handling

The system detects and refuses three types of out-of-scope questions:

### 3.1 Future Predictions (Q11)
```
Pattern: "will be", "in 2025", "forecast", "predict"
Response: "This question requires speculation about future events..."
```

### 3.2 Investment Advice (Q12)
```
Pattern: "should I invest", "buy stock", "recommend"
Response: "I cannot provide investment advice..."
```

### 3.3 External Data (Q13)
```
Pattern: "compare to Microsoft", "versus Google"
Response: "This question requires data not present in the provided documents..."
```

**Implementation:** Regex patterns in `_is_out_of_scope()` + LLM prompt instructions for refusal.

---

## 4. Citation Grounding

Every answer includes verifiable citations:

```json
{
  "answer": "Apple's total revenue was $391,035 million",
  "sources": ["Apple 10-K", "Item 7", "p. 25"]
}
```

**How it works:**
1. Each chunk carries metadata: `{document, section, page_start, page_end}`
2. LLM is prompted to cite sources in specific format
3. `parse_answer_and_sources()` extracts citations from response
4. Citations are validated against retrieved chunks

---

## 5. Performance

| Metric | Value |
|--------|-------|
| Indexing time | ~30 seconds (491 chunks) |
| Query latency | ~2-3 seconds |
| Accuracy (13 questions) | 12/13 correct |
| Out-of-scope detection | 2/3 refused properly |

---

## 6. Limitations & Future Work

1. **Table extraction:** Complex multi-page tables occasionally lose structure
2. **Numbers:** Very specific numerical queries sometimes need exact BM25 match
3. **Q12 partial refusal:** Investment advice question leaked some context
4. **No caching:** Each query re-embeds (could cache common queries)

**If I had more time:**
- Add query expansion for financial synonyms
- Implement table-aware chunking
- Add evaluation metrics (ROUGE, BERTScore)
- Fine-tune reranker on SEC filings

---

## 7. Running the System

```bash
# Install dependencies
pip install -r requirements.txt

# Set API key
echo "GROQ_API_KEY=your_key" > .env

# Run evaluation
python -m src.test.evaluate

# Or use programmatically
from src.pipeline import RAGPipeline
pipeline = RAGPipeline(llm_provider="groq")
pipeline.index_documents("data/")
result = pipeline.answer_question("What was Apple's revenue in 2024?")
```

---

*This system demonstrates a production-ready RAG architecture for financial document QA, built entirely with open-source components.*
